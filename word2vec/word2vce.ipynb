{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "---\n",
    "Word2Vec 是一种著名的 词嵌入（Word Embedding） 方法，它可以计算每个单词在其给定语料库环境下的 分布式词向量（Distributed Representation，亦直接被称为词向量）。词向量表示可以在一定程度上刻画每个单词的语义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单用法\n",
    "---\n",
    "### 读取语料\n",
    "---\n",
    "* class gensim.models.word2vec.BrownCorpus（dirname ）\n",
    "从布朗语料库（NLTK数据的一部分）迭代句子,dirname是存储布朗语料库的根目录(通过nltk.download()下载布朗语料库)，得到的这个对象可以通过循环迭代语料库的句子。\n",
    "\n",
    "* class gensim.models.word2vec.LineSentence(source, max_sentence_length=10000, limit=None)\n",
    "与上一样，也是产生迭代器，但需要更改下文件格式。简单的格式：一篇文档=一行; 单词已经过预处理并由空格分隔。\n",
    "\n",
    "* class gensim.models.word2vec.PathLineSentences（source，max_sentence_length = 10000，limit = None ）\n",
    "与LineSentence类一样，不过这里是处理根目录下的所有文件，同样文件中句子格式需要处理\n",
    "\n",
    "* class gensim.models.word2vec.Text8Corpus（fname，max_sentence_length = 10000 ）\n",
    "从text8语料库中迭代句子\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "file_path = 'word2vec训练数据.txt'\n",
    "# 使用LineSentence读取语料\n",
    "sentences = word2vec.LineSentence(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练word2vec语义向量\n",
    "---\n",
    "```python\n",
    "class gensim.models.word2vec.Word2Vec(sentences=None, size=100, alpha=0.025, window=5, min_count=5,  \n",
    "                   max_vocab_size=None, sample=1e-3, seed=1, workers=3, min_alpha=0.0001,  \n",
    "                   sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=hash, iter=5, null_word=0,  \n",
    "                   trim_rule=None, sorted_vocab=1, batch_words=MAX_WORDS_IN_BATCH, compute_loss=False, callbacks=(),  \n",
    "                   max_final_vocab=None)  \n",
    "```\n",
    "* sentence(iterable of iterables):可迭代的句子可以是简单的list，但对于较大的语料库，可以考虑直接从磁盘/网络传输句子的迭代。见BrownCorpus，Text8Corpus 或LineSentence.\n",
    "* SG(INT {1 ，0}) -定义的训练算法。如果是1，则使用skip-gram; 否则，使用CBOW。\n",
    "* hs：是否采用基于Hierarchical Softmax的模型。参数为1表示使用，0表示不使用\n",
    "* size(int) - 特征向量的维数。\n",
    "* window(int) - 句子中当前词和预测词之间的最大距离。\n",
    "* min_count(int) - 忽略总频率低于此值的所有单词。 \n",
    "\n",
    "    关于Hierarchical Softmax与negative sampling，可以参考以下博客:  \n",
    "        http://www.cnblogs.com/pinard/p/7243513.html  \n",
    "        https://www.cnblogs.com/pinard/p/7249903.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, hs=1,min_count=1,window=2,size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型\n",
    "---\n",
    "model.save(file_name)\n",
    "* file_name:存储模型的名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_word2vec_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型\n",
    "---\n",
    "word2vec.Word2Vec.load(file_name)\n",
    "* file_name:存储的模型的名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load('model_word2vec_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.49542701e-02,   8.05083513e-02,  -2.86384404e-01,\n",
       "        -2.28744335e-02,  -5.72136082e-02,   6.77325651e-02,\n",
       "        -1.92947462e-01,  -1.59176052e-01,  -3.13154794e-02,\n",
       "        -8.99701789e-02,  -2.91670468e-02,  -1.11418039e-01,\n",
       "         2.80972064e-01,  -8.18192363e-02,  -9.02986154e-02,\n",
       "         2.09552854e-01,   6.51546046e-02,  -1.98360413e-01,\n",
       "         1.95879802e-01,   4.65310663e-01,  -2.06638277e-01,\n",
       "        -3.28917317e-02,   3.41773555e-02,   1.33826151e-01,\n",
       "        -6.09533414e-02,   3.34803939e-01,   3.37274104e-01,\n",
       "         1.51200667e-01,  -9.53703672e-02,  -2.59918690e-01,\n",
       "        -2.82655358e-01,  -2.11445779e-01,   1.35404002e-02,\n",
       "         1.15187936e-01,  -7.70035312e-02,   2.89677113e-01,\n",
       "        -1.09858043e-01,  -3.03654850e-01,  -8.24007317e-02,\n",
       "         1.04838744e-01,   3.38284560e-02,   8.95413086e-02,\n",
       "        -1.22524180e-01,  -9.29293633e-02,   1.51894912e-01,\n",
       "        -2.36794010e-01,   8.58983994e-02,  -2.16335282e-01,\n",
       "         1.18892707e-01,   2.86993146e-01,   1.71657547e-01,\n",
       "        -2.16277614e-01,  -8.07731599e-02,   3.16066712e-01,\n",
       "         3.75020318e-02,  -6.78976672e-03,  -1.79260835e-01,\n",
       "         6.57181069e-02,   5.14917850e-01,   3.90850082e-02,\n",
       "        -2.77069956e-01,   2.55487654e-02,   1.10175824e-02,\n",
       "        -8.30561146e-02,  -4.92776446e-02,  -9.04060826e-02,\n",
       "         1.38659373e-01,  -2.48618335e-01,  -1.19901016e-01,\n",
       "         9.75399613e-02,  -2.03123033e-01,   1.03411265e-01,\n",
       "        -1.42611563e-01,  -1.64072633e-01,  -3.24298330e-02,\n",
       "         4.11537301e-04,  -4.33171630e-01,   1.61016788e-02,\n",
       "        -1.60183862e-01,  -2.24332083e-02,   9.03616250e-02,\n",
       "         4.45240289e-02,  -1.20245717e-01,  -1.11933969e-01,\n",
       "         3.05113673e-01,  -6.08405396e-02,  -1.88946590e-01,\n",
       "         2.65322197e-02,  -1.73022270e-01,   6.35778978e-02,\n",
       "        -4.86270040e-02,   1.34485781e-01,   2.18672261e-01,\n",
       "         2.44026661e-01,  -4.00636524e-01,   6.84009045e-02,\n",
       "        -1.00347050e-01,  -7.14292824e-02,  -1.71577334e-01,\n",
       "        -1.84371039e-01,  -3.08488868e-02,   2.47588515e-01,\n",
       "         1.71582296e-01,  -2.17009112e-01,   3.66187006e-01,\n",
       "        -6.04885742e-02,  -3.03210050e-01,   1.15487970e-01,\n",
       "        -5.94740689e-01,  -1.62364855e-01,  -9.98616740e-02,\n",
       "        -2.19380613e-02,  -1.44833863e-01,   1.93577603e-01,\n",
       "        -7.83838630e-02,   3.75213414e-01,   1.35010004e-01,\n",
       "         4.97069448e-01,   8.01630244e-02,  -4.25554752e-01,\n",
       "         2.00448826e-01,  -6.58988729e-02,   1.39411464e-01,\n",
       "        -1.16766348e-01,   2.93897122e-01,   3.41877550e-01,\n",
       "         6.30570501e-02,   1.06310733e-02], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取单词word2vec值\n",
    "model['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.617068983169\n",
      "0.981825965193\n",
      "0.983594876016\n"
     ]
    }
   ],
   "source": [
    "# 计算两个单词的语义相似度\n",
    "print(model.similarity('魅族','全网通'))\n",
    "print(model.similarity('16g','64g'))\n",
    "print(model.similarity('粉色','金色'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网上中文语料库\n",
    "腾讯AI实验室宣布，正式开源一个大规模、高质量的中文词向量数据集  \n",
    "https://ai.tencent.com/ailab/nlp/embedding.html  \n",
    "120G+训练好的word2vec模型（中文词向量）  \n",
    "https://blog.csdn.net/tu_22/article/details/79035769  \n",
    "还有一些开源语料库，可以自己拿去训练。。。。。。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
